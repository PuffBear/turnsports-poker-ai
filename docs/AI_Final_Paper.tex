\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{cite}
\usepackage{caption}
\usepackage{subcaption}

% Code listing style
\lstset{
    language=Python,
    basicstyle=\ttfamily\small,
    keywordstyle=\color{blue},
    commentstyle=\color{gray},
    stringstyle=\color{orange},
    numbers=left,
    numberstyle=\tiny,
    frame=single,
    breaklines=true,
    captionpos=b
}

% Title and author
\title{\textbf{An Intelligent Poker AI System:} \\ Combining Counterfactual Regret Minimization with LLM-Powered Coaching for Heads-Up No-Limit Texas Hold'em}

\author{Agriya Yadav \\ 
Department of Computer Science \\
Ashoka University \\
\texttt{agriya.yadav_ug2023@ashoka.edu.in}}

\date{\today}

\begin{document}

\maketitle

\begin{abstract}
This paper presents the development of a complete poker AI system for Heads-Up No-Limit Texas Hold'em that combines game-theoretic training via Counterfactual Regret Minimization (CFR) with an LLM-powered coaching interface. The system achieves near-Nash equilibrium play through K-Means card abstraction and extensive self-play training, reaching an exploitability of 0.17 BB/hand after 25,000 iterations. We integrate this with a natural language coaching system powered by Ollama's Llama 3.2 model, providing real-time strategic guidance with equity analysis, board texture recognition, and opponent modeling. Our implementation demonstrates that meaningful poker AI can be developed with consumer hardware through careful abstraction techniques, achieving performance comparable to research-level systems. The complete system includes a GUI for human-vs-AI play with integrated coaching, session analytics, and strategic recommendations across multiple risk tolerance levels.

\textbf{Keywords:} Poker AI, Counterfactual Regret Minimization, Game Theory, LLM Agents, K-Means Clustering, Imperfect Information Games
\end{abstract}

\newpage
\tableofcontents
\newpage

\section{Introduction}

\subsection{Motivation and Background}

Poker represents one of the most challenging domains in artificial intelligence due to its nature as an imperfect information game. Unlike perfect information games such as chess or Go, poker requires agents to reason about hidden information, deception, and probabilistic outcomes. The complexity of Heads-Up No-Limit Texas Hold'em (HUNL) has made it a benchmark problem in AI research, with the game space containing approximately $10^{160}$ possible game states.

The motivation for this project stems from three key observations:
\begin{enumerate}
    \item \textbf{Theoretical Foundation:} Poker is solvable in principle through game-theoretic equilibrium strategies, providing a clear target for algorithmic approaches.
    \item \textbf{Educational Value:} An AI poker coach could help players improve their strategies through real-time analysis and strategic feedback.
    \item \textbf{Technical Challenge:} Building a complete poker AI system requires integrating multiple complex components: game theory algorithms, abstraction techniques, user interfaces, and modern LLM capabilities.
\end{enumerate}

This project aims to create not just a strong poker bot, but a complete ecosystem that combines competitive play with educational coaching, making advanced poker strategy accessible through natural language interaction.

\subsection{Project Objectives}

The primary objectives of this project are:

\begin{enumerate}
    \item \textbf{Implement a Game-Theoretically Sound Poker Bot:} Develop a CFR-based agent capable of near-optimal play in HUNL through abstraction and extensive training.
    
    \item \textbf{Create Meaningful Card Abstractions:} Move beyond simple bucketing to K-Means clustering based on equity distributions, ensuring the bot learns strategic rather than arbitrary patterns.
    
    \item \textbf{Build an LLM-Powered Coaching System:} Integrate modern language models to provide contextual, natural language poker advice that combines equity calculations with strategic reasoning.
    
    \item \textbf{Deliver a Complete User Experience:} Create an intuitive GUI that allows humans to play against the bot while receiving real-time coaching, tracking session statistics, and understanding strategic decisions.
    
    \item \textbf{Optimize for Consumer Hardware:} Design the system to train and run effectively on standard computing resources through algorithmic optimizations.
\end{enumerate}

\subsection{Related Work}

The field of computer poker has seen significant advances in recent years:

\textbf{Libratus (2017):} Developed by CMU's Tuomas Sandholm and Noam Brown, Libratus defeated top professional poker players in HUNL. It used CFR+ with abstraction refinement and achieved near-perfect play through massive computational resources (15 million core-hours) \cite{brown2017libratus}.

\textbf{Pluribus (2019):} Extended poker AI to 6-player No-Limit Hold'em, considered a significantly harder problem. Pluribus used Monte Carlo CFR with a depth-limited search and achieved superhuman performance with much less computation than Libratus \cite{brown2019pluribus}.

\textbf{DeepStack (2017):} Combined deep learning with game theory, using neural networks to evaluate positions and CFR for strategic planning. It was the first bot to beat professional players in HUNL \cite{moravcik2017deepstack}.

\textbf{Key Differences in Our Approach:}
\begin{itemize}
    \item \textit{Accessibility:} Designed to run on consumer hardware rather than supercomputers
    \item \textit{Educational Focus:} Integrated LLM coaching for player improvement
    \item \textit{Transparency:} Explainable recommendations through natural language
    \item \textit{Modern Integration:} Leverages recent LLM advances (Ollama/Llama 3.2)
\end{itemize}

\subsection{Report Structure}

This paper is organized as follows: Section 2 covers the core poker mechanics and environment implementation. Section 3 details the system architecture and component integration. Section 4 examines the CFR training process and K-Means abstraction. Section 5 describes the agentic coaching system. Section 6 explains the GUI implementation. Section 7 presents results and evaluation. Section 8 discusses challenges and limitations. Section 9 outlines future work, and Section 10 concludes.

\section{Core Poker Mechanics}

\subsection{Heads-Up No-Limit Hold'em Rules}

HUNL is played between two players with the following structure:
\begin{itemize}
    \item Each player receives two private cards (hole cards)
    \item Five community cards are dealt in stages: Flop (3), Turn (1), River (1)
    \item Four betting rounds: Preflop, Flop, Turn, River
    \item No betting limit (players can bet any amount up to their stack)
    \item Best 5-card hand wins at showdown
\end{itemize}

The game's complexity arises from the betting structure. At any decision point, a player can:
\begin{enumerate}
    \item \textbf{Fold:} Forfeit the hand
    \item \textbf{Check/Call:} Match opponent's bet or check if no bet
    \item \textbf{Raise:} Increase the bet (minimum raise, fractional pot sizes, all-in)
\end{enumerate}

This creates a decision tree with branching factor dependent on stack sizes and pot size, leading to astronomical state space complexity.

\subsection{The HoldemHuEnv Implementation}

We implemented the game environment following the OpenAI Gymnasium interface for compatibility with reinforcement learning frameworks.

\subsubsection{State Representation}

The environment maintains the complete game state:
\begin{lstlisting}[caption=Core State Variables in HoldemHuEnv]
class HoldemHuEnv:
    def __init__(self, stack_size=100.0, blinds=(0.5, 1.0)):
        self.initial_stack = stack_size
        self.sb_amount, self.bb_amount = blinds
        
        # Game state
        self.deck = Deck()
        self.hands = []  # 2-element list of hand cards
        self.board = []  # Community cards
        self.stacks = []  # Current stacks
        self.pot = 0
        self.street = 0  # 0=Preflop, 1=Flop, 2=Turn, 3=River
        self.street_investment = []  # Chips put in this street
        self.current_player = 0
        self.done = False
\end{lstlisting}

\subsubsection{Action Space}

The environment supports 9 discrete actions:
\begin{align*}
    \mathcal{A} = \{&\text{Fold}, \text{Check/Call}, \text{Min Raise}, \\
    &\text{0.25 Pot}, \text{0.33 Pot}, \text{0.5 Pot}, \\
    &\text{0.75 Pot}, \text{Pot}, \text{All-In}\}
\end{align*}

This granularity balances expressiveness with computational tractability.

\subsubsection{Observation Space}

Observations are 200-dimensional vectors encoding:
\begin{itemize}
    \item Hole cards (52-dimensional one-hot encoding)
    \item Board cards (52-dimensional one-hot encoding × 5)
    \item Position (button indicator)
    \item Pot size and stack sizes (normalized)
    \item Street investments (current betting round state)
    \item Street indicator (one-hot encoding)
    \item Action history features
\end{itemize}

\subsection{Card Representation}

\subsubsection{The Card Object}

Cards are represented as immutable objects with rank and suit:
\begin{lstlisting}[caption=Card Class Implementation]
class Card:
    RANKS = ['2', '3', '4', '5', '6', '7', '8', '9', 
             'T', 'J', 'Q', 'K', 'A']
    SUITS = ['s', 'h', 'd', 'c']  # spades, hearts, diamonds, clubs
    
    def __init__(self, rank, suit):
        if rank not in self.RANKS or suit not in self.SUITS:
            raise ValueError(f"Invalid card: {rank}{suit}")
        self.rank = rank
        self.suit = suit
    
    def __str__(self):
        return f"{self.rank}{self.suit}"
\end{lstlisting}

\subsubsection{The Deck}

The Deck manages the 52-card state:
\begin{lstlisting}[caption=Deck Implementation]
class Deck:
    def __init__(self):
        self.cards = [Card(r, s) for s in Card.SUITS 
                      for r in Card.RANKS]
        self.shuffle()
    
    def shuffle(self):
        random.shuffle(self.cards)
    
    def draw(self, n=1):
        if n > len(self.cards):
            raise ValueError("Not enough cards in deck")
        drawn = self.cards[:n]
        self.cards = self.cards[n:]
        return drawn
\end{lstlisting}

\subsection{Hand Evaluation}

Hand strength is crucial for both bot decisions and showdown determination. We implemented a 7-card hand evaluator using a lookup table approach.

\subsubsection{Hand Rankings}

Standard 5-card poker hands from weakest to strongest:
\begin{enumerate}
    \item High Card
    \item One Pair
    \item Two Pair
    \item Three of a Kind
    \item Straight
    \item Flush
    \item Full House
    \item Four of a Kind
    \item Straight Flush
\end{enumerate}

\subsubsection{Evaluation Algorithm}

The evaluator returns an integer score where \textit{lower is better}:
\begin{lstlisting}[caption=Hand Evaluator Core Logic]
class HandEvaluator:
    @staticmethod
    def evaluate(cards):
        """
        Evaluate 5-7 cards, return integer score.
        Lower score = stronger hand.
        """
        if len(cards) == 5:
            return HandEvaluator._eval_5cards(cards)
        
        # For 7 cards, find best 5-card combination
        best_score = float('inf')
        for combo in combinations(cards, 5):
            score = HandEvaluator._eval_5cards(combo)
            best_score = min(best_score, score)
        return best_score
\end{lstlisting}

\subsection{Equity Estimation}

Equity represents the probability of winning given current cards and board state. We use Monte Carlo simulation:

\begin{algorithm}[H]
\caption{Monte Carlo Equity Estimation}
\begin{algorithmic}
\STATE \textbf{Input:} $h$ (our hand), $b$ (board), $n$ (samples)
\STATE \textbf{Output:} Equity estimate $\in [0,1]$
\STATE wins $\leftarrow$ 0
\FOR{$i = 1$ to $n$}
    \STATE Sample random opponent hand $h_{opp}$
    \STATE Complete board with random cards $b_{complete}$
    \STATE $score_{us} \leftarrow$ HandEval($h + b_{complete}$)
    \STATE $score_{opp} \leftarrow$ HandEval($h_{opp} + b_{complete}$)
    \IF{$score_{us} < score_{opp}$}
        \STATE wins $\leftarrow$ wins + 1
    \ENDIF
\ENDFOR
\RETURN wins / $n$
\end{algorithmic}
\end{algorithm}

For our system, we use $n = 5000$ samples for coach recommendations, balancing accuracy (standard error $\approx 0.7\%$) with response time ($\approx 0.5s$).

\section{System Architecture}

The poker AI system consists of multiple interconnected components, each responsible for specific functionality. Figure~\ref{fig:architecture} shows the high-level architecture.

\subsection{Component Overview}

\subsubsection{Core Components}
\begin{enumerate}
    \item \textbf{HoldemHuEnv:} Game engine enforcing poker rules
    \item \textbf{CFRAgent:} Game-theoretic bot using CFR training
    \item \textbf{CardAbstraction:} Bucketing hands to reduce state space
    \item \textbf{ActionAbstraction:} Simplifying bet sizing
    \item \textbf{AgenticCoach:} Strategic analysis engine
    \item \textbf{LLMCoach:} Natural language recommendation system
    \item \textbf{GUI:} Tkinter-based interface
\end{enumerate}

\subsection{Data Flow Architecture}

\subsubsection{GUI $\rightarrow$ Game Components}

When a user takes an action:
\begin{enumerate}
    \item GUI validates action legality
    \item Call \texttt{env.step(action\_id)}
    \item Environment updates game state
    \item Environment returns (observation, reward, done, info)
    \item GUI updates display
\end{enumerate}

\subsubsection{CFR Agent Integration}

The CFR agent operates through abstraction layers:
\begin{lstlisting}[caption=CFR Agent Action Selection]
def get_action(self, env, player_idx):
    # Get abstracted info set
    info_set = self._get_info_set(env, player_idx)
    
    # info_set format: "bucket|street|pot|to_call"
    # Example: "42|turn|150|50"
    
    # Retrieve average strategy (equilibrium)
    strategy = self.get_average_strategy(
        info_set, 
        env._get_legal_actions()
    )
    
    # Sample action from strategy
    abstract_action = np.random.choice(
        self.num_actions, 
        p=strategy
    )
    
    # Map abstract -> environment action
    return self.abstract_to_env[abstract_action]
\end{lstlisting}

\subsubsection{Coach $\rightarrow$ Abstractions}

The coach uses the same abstractions for consistency:
\begin{enumerate}
    \item \textbf{Card Abstraction:} Computes hand bucket for equity distribution
    \item \textbf{Equity Tools:} Calculates win probability via Monte Carlo
    \item \textbf{State Summary:} Extracts strategic features (SPR, position, pot odds)
    \item \textbf{LLM Integration:} Formats analysis into natural language
\end{enumerate}

\subsection{Training Pipeline}

The training process is separate from runtime:

\begin{lstlisting}[caption=CFR Training Flow]
def train_abstracted_cfr(n_iterations=25000):
    # 1. Load K-Means models
    card_abs = CardAbstraction()
    card_abs.load_kmeans_models(
        'kmeans_flop.pkl', 
        'kmeans_turn.pkl'
    )
    
    # 2. Initialize CFR agent
    agent = CFRAgent(
        card_abstraction=card_abs,
        action_abstraction=ActionAbstraction(),
        num_actions=5
    )
    
    # 3. Self-play training
    for i in range(n_iterations):
        env.reset()
        # Alternate between player perspectives
        player = i % 2
        agent._external_sampling_cfr(env, player)
        
        # Periodic evaluation and checkpoint saving
        if i % 5000 == 0:
            evaluate_exploitability(agent)
            agent.save(f'checkpoint_{i}.pkl')
    
    return agent
\end{lstlisting}

\section{Training CFR with Abstraction}

\subsection{Counterfactual Regret Minimization}

CFR is an iterative algorithm that converges to Nash equilibrium in two-player zero-sum games. The key insight is \textit{regret matching}: play actions proportionally to how much you regret not having played them.

\subsubsection{Counterfactual Values}

For each information set $I$ and action $a$, we compute:
\begin{equation}
    v(\sigma, I, a) = \sum_{h \in I, h' \in Z} \pi^{-i}(h)\pi^{\sigma}(h, h')u^i(h')
\end{equation}

Where:
\begin{itemize}
    \item $\sigma$ is the current strategy profile
    \item $\pi^{-i}(h)$ is the reach probability by opponent
    \item $u^i(h')$ is utility at terminal history $h'$
\end{itemize}

\subsubsection{Regret and Strategy Update}

Instantaneous regret for action $a$ at information set $I$:
\begin{equation}
    r^T(I, a) = v(\sigma^T, I, a) - v(\sigma^T, I)
\end{equation}

Cumulative regret:
\begin{equation}
    R^T(I, a) = \sum_{t=1}^{T} r^t(I, a)
\end{equation}

Next strategy via regret matching:
\begin{equation}
    \sigma^{T+1}(I, a) = \begin{cases}
        \frac{R^T_+(I,a)}{\sum_{a' \in A(I)} R^T_+(I,a')} & \text{if denominator} > 0 \\
        \frac{1}{|A(I)|} & \text{otherwise}
    \end{cases}
\end{equation}

Where $R^T_+(I,a) = \max(R^T(I,a), 0)$.

\subsection{External Sampling CFR}

We use external sampling CFR, which samples opponent actions for efficiency:

\begin{algorithm}[H]
\caption{External Sampling CFR}
\begin{algorithmic}
\STATE \textbf{Input:} Environment $env$, player $i$
\STATE \textbf{Output:} Expected utility for player $i$

\IF{$env.done$}
    \RETURN $env.rewards[i]$
\ENDIF

\STATE $I \leftarrow$ GetInfoSet($env$, $current\_player$)
\STATE $\sigma \leftarrow$ GetStrategy($I$)

\IF{$current\_player = i$}
    \STATE Save environment state
    \STATE $util \leftarrow \vec{0}$
    \FOR{each action $a$ in legal actions}
        \STATE Restore environment
        \STATE Execute action $a$
        \STATE $util[a] \leftarrow$ ExternalCFR($env$, $i$)
    \ENDFOR
    \STATE $v \leftarrow \sigma \cdot util$
    \STATE $regret \leftarrow util - v$
    \STATE Update cumulative regret and strategy
    \RETURN $v$
\ELSE
    \STATE Sample action $a \sim \sigma$
    \STATE Execute action $a$
    \RETURN ExternalCFR($env$, $i$)
\ENDIF
\end{algorithmic}
\end{algorithm}

\subsection{K-Means Card Abstraction}

Card abstraction is critical for tractability. We use K-Means clustering on equity distribution histograms.

\subsubsection{Motivation}

Simple equity bucketing (grouping hands by average equity) loses critical information. Consider:
\begin{itemize}
    \item \textbf{Made Hand (AA):} 80\% equity, tight distribution
    \item \textbf{Draw (AKs on QJ9):} 45\% equity, bimodal distribution
\end{itemize}

Both might have similar equity but require completely different strategies. Equity \textit{distribution} captures this.

\subsubsection{Equity Distribution Histogram}

For each hand $h$ and board $b$, we compute:
\begin{enumerate}
    \item Sample $n$ Monte Carlo simulations
    \item For each simulation, record final equity (0.0 or 1.0)
    \item Create histogram with 50 bins $\in [0, 1]$
    \item Result is 50-dimensional feature vector
\end{enumerate}

Example:
\begin{lstlisting}[caption=Equity Distribution Calculation]
def calc_equity_distribution(hole, board, 
                             n_bins=50, n_samples=200):
    equities = []
    for _ in range(n_samples):
        # Sample random opponent hand
        opp_hand = sample_hand(exclude=hole+board)
        # Complete board randomly
        full_board = complete_board(board)
        # Did we win?
        win = evaluate(hole + full_board) < 
              evaluate(opp_hand + full_board)
        equities.append(1.0 if win else 0.0)
    
    # Create histogram
    hist, _ = np.histogram(equities, bins=n_bins, 
                           range=(0,1))
    return hist / n_samples  # Normalize
\end{lstlisting}

\subsubsection{K-Means Clustering}

We train separate K-Means models for flop and turn:

\begin{algorithm}[H]
\caption{Generate K-Means Clusters}
\begin{algorithmic}
\STATE \textbf{Input:} Street (flop/turn), $k$ clusters, $n_{samples}$ hands
\STATE \textbf{Output:} Trained K-Means model

\STATE samples $\leftarrow []$
\FOR{$i = 1$ to $n_{samples}$}
    \STATE Deal random hand and board for street
    \STATE $hist \leftarrow$ CalcEquityDistribution(hand, board)
    \STATE samples.append($hist$)
\ENDFOR

\STATE kmeans $\leftarrow$ KMeans($k$ clusters)
\STATE kmeans.fit(samples)
\RETURN kmeans
\end{algorithmic}
\end{algorithm}

\textbf{Our Configuration:}
\begin{itemize}
    \item Preflop: 169 buckets (lossless - hand equivalence classes)
    \item Flop: 50 K-Means clusters
    \item Turn: 50 K-Means clusters
    \item River: 10 equity buckets (small because game almost over)
\end{itemize}

\textbf{Result:} Total of ~4.2 million possible information sets, down from $10^{160}$.

\subsection{Action Abstraction}

We reduce 9 environment actions to 5 abstract actions:

\begin{center}
\begin{tabular}{|c|l|l|}
\hline
\textbf{Abstract} & \textbf{Name} & \textbf{Env Mapping} \\
\hline
0 & Fold & 0 (Fold) \\
1 & Check/Call & 1 (Check/Call) \\
2 & Small Bet & 4 (1/3 pot) \\
3 & Large Bet & 6 (3/4 pot) \\
4 & Overbet & 7 (Pot-sized) \\
\hline
\end{tabular}
\end{center}

Additionally, we limit betting sequences to 11 predefined valid sequences per street to avoid infinite bet/raise spirals.

\subsection{Training Process and Results}

\subsubsection{Training Configuration}

\begin{itemize}
    \item \textbf{Iterations:} 25,000 (self-play hands)
    \item \textbf{Time:} ~8 hours on consumer hardware (M1 Mac)
    \item \textbf{Memory:} Final model size 3.5 MB
    \item \textbf{Checkpoints:} Saved every 5,000 iterations
\end{itemize}

\subsubsection{Optimizations}

Two critical optimizations enabled feasible training:

\textbf{1. Minimal State Copying:}
\begin{lstlisting}[caption=Optimized State Save/Restore]
def _save_env_state(self, env):
    """Save only necessary state - NOT entire objects"""
    return {
        'hands': [h[:] for h in env.hands],  # Shallow copy
        'board': env.board[:],
        'deck_cards': env.deck.cards[:],  # Just card list
        'pot': env.pot,
        'stacks': env.stacks[:],
        'street_investment': env.street_investment[:],
        'street': env.street,
        'current_player': env.current_player,
        'done': env.done,
        'has_acted': env.has_acted[:]
    }
\end{lstlisting}

This reduced state copy time from ~100ms to ~1ms, a 100x speedup.

\textbf{2. Reduced Equity Samples:}
During training, we use only 20 Monte Carlo samples (vs 5000 during play) for equity distribution calculation. This trades minor accuracy for 250x speedup during clustering.

\subsubsection{Convergence Analysis}

Figure~\ref{fig:training_curves} shows exploitability over training. Key observations:

\begin{itemize}
    \item \textbf{Initial exploitability:} ~2.5 BB/hand (iteration 1000)
    \item \textbf{Best exploitability:} 0.17 BB/hand (iteration 24,500)
    \item \textbf{Final exploitability:} 0.75 BB/hand (iteration 25,000)
    \item \textbf{Info sets learned:} 45,962 unique situations
\end{itemize}

The oscillation is expected - CFR explores different strategies before converging. The trend is clearly downward, indicating movement toward Nash equilibrium.

\section{Agentic AI Coach}

\subsection{Architecture Overview}

The coaching system consists of two layers:

\begin{enumerate}
    \item \textbf{AgenticCoach:} Tool-wielding analysis engine
    \item \textbf{LLMCoach:} Natural language interface via Ollama
\end{enumerate}

\subsection{Tool-Based Analysis}

The AgenticCoach uses specialized tools:

\subsubsection{Equity Tools}
\begin{lstlisting}[caption=Equity Estimation Tool]
def estimate_equity(hand, board, n_samples=5000):
    """Monte Carlo equity estimation"""
    wins = 0
    for _ in range(n_samples):
        # Sample opponent hand
        opp = sample_opponent_hand(exclude=hand+board)
        # Complete board
        full_board = complete_board(board)
        # Compare hands
        if hand_wins(hand, opp, full_board):
            wins += 1
    return wins / n_samples
\end{lstlisting}

\subsubsection{Pot Odds Calculation}
\begin{equation}
    \text{Pot Odds} = \frac{\text{Amount to Call}}{\text{Pot} + \text{Amount to Call}}
\end{equation}

Decision rule: Call if $\text{Equity} > \text{Pot Odds}$.

\subsubsection{State Summary}
Extracts strategic features:
\begin{itemize}
    \item SPR (Stack-to-Pot Ratio)
    \item Position (IP/OOP)
    \item Street (Preflop/Flop/Turn/River)
    \item Board texture (wet/dry/coordinated)
\end{itemize}

\subsection{LLM Integration}

We use Ollama with Llama 3.2 for natural language generation.

\subsubsection{Enhanced Prompt Structure}

The prompt is carefully structured for maximum clarity:

\begin{lstlisting}[caption=LLM Prompt Structure (Simplified)]
prompt = f"""
You are a professional No-Limit Hold'em poker coach.

=== HAND SITUATION ===
Street: {street}
Your Hand: {hand}
Board: {board}
Position: {position} (IN POSITION / OUT OF POSITION)

=== STACK & POT INFO ===
Pot Size: {pot} BB
To Call: {to_call} BB
SPR: {spr}

=== BOARD TEXTURE ===
Type: {texture}  # wet/dry/coordinated
Threats: {draws}  # flush draw, straight draw, etc.

=== EQUITY ANALYSIS ===
Win Probability: {equity}%
Required Equity: {pot_odds}%
Edge: {equity - pot_odds}%

=== STRATEGIC FACTORS ===
• SPR Implications: {spr_note}
• Position Advantage<br/>: {position_note}

Provide 3-4 sentence recommendation covering:
1. Recommended action and why
2. Why better than alternatives
3. Key strategic consideration
4. Future street planning
"""
\end{lstlisting}

\subsubsection{Board Texture Analysis}

We automatically analyze board coordination:
\begin{lstlisting}[caption=Board Texture Detection]
def analyze_board_texture(board):
    # Check flush draw (3+ same suit)
    suit_counts = count_suits(board)
    has_flush_draw = max(suit_counts.values()) >= 3
    
    # Check straight possibilities
    ranks = get_rank_values(board)
    has_straight_draw = max(ranks) - min(ranks) <= 4
    
    # Check pairs
    rank_counts = count_ranks(board)
    has_pair = any(count >= 2 for count in rank_counts.values())
    
    # Classify
    threats = []
    if has_flush_draw: threats.append('flush draw')
    if has_straight_draw: threats.append('straight draw')
    if has_pair: threats.append('trips possible')
    
    if len(threats) >= 2:
        return 'wet', threats
    elif len(threats) == 1:
        return 'semi-wet', threats
    else:
        return 'dry', []
\end{lstlisting}

\subsection{Risk Tolerance Levels}

The coach supports three risk profiles:

\begin{enumerate}
    \item \textbf{Conservative:} Requires large equity edges, folds marginal spots
    \item \textbf{Moderate:} Balanced approach, small edges acceptable
    \item \textbf{Aggressive:} Exploitative, willing to bluff with thin edges
\end{enumerate}

\begin{lstlisting}[caption=Risk-Based Recommendations]
def recommend_for_risk(equity, pot_odds, risk_level):
    if risk_level == 'conservative':
        # Need 20% edge over pot odds
        return 'call' if equity > pot_odds + 0.20 else 'fold'
    
    elif risk_level == 'moderate':
        # Need 10% edge
        return 'call' if equity > pot_odds + 0.10 else 'fold'
    
    else:  # aggressive
        # Accept slight deficit for pressure
        return 'call' if equity > pot_odds - 0.05 else 'fold'
\end{lstlisting}

\subsection{Opponent Modeling}

The system tracks opponent tendencies:
\begin{itemize}
    \item VPIP (Voluntarily Put money In Pot)
    \item PFR (Pre-Flop Raise frequency)
    \item Aggression factor
    \item Fold to c-bet percentage
\end{itemize}

These stats feed into recommendations after sufficient hand history.

\subsection{Framework and Planning}

\textbf{Is this "agentic"?} 

Yes, in the tool-use sense:
\begin{enumerate}
    \item \textbf{Tool Selection:} Coach decides which tools to invoke (equity, pot odds, board analysis)
    \item \textbf{Reasoning:} Combines tool outputs into coherent recommendation
    \item \textbf{Adaptation:} Adjusts to risk tolerance and opponent stats
\end{enumerate}

\textbf{No explicit planning} (like Monte Carlo Tree Search), but the structured analysis approximates forward-looking strategic reasoning.

\section{Graphical User Interface}

\subsection{Technology Stack}

\begin{itemize}
    \item \textbf{Framework:} Python Tkinter (cross-platform GUI toolkit)
    \item \textbf{Layout:} Grid-based with responsive weights
    \item \textbf{Styling:} ttk themed widgets for modern appearance
\end{itemize}

\subsection{GUI Components}

\subsubsection{Layout Structure}

Two-panel design:
\begin{enumerate}
    \item \textbf{Left Panel (Game):}
    \begin{itemize}
        \item Board/pot/street display
        \item Player and bot information (hand, stack, role)
        \item Action history log
        \item Action buttons (9 bet sizes)
        \item Session statistics table
    \end{itemize}
    
    \item \textbf{Right Panel (Coach):}
    \begin{itemize}
        \item Risk tolerance selector
        \item Statistics display (pot, equity, SPR)
        \item LLM recommendation text
        \item All risk levels comparison
    \end{itemize}
\end{enumerate}

\subsubsection{Key Features}

\textbf{1. Real-Time Recommendations:}
Triggered when it's the player's turn:
\begin{lstlisting}[caption=Recommendation Update Logic]
def update_recommendations(self):
    if not self.env.done and 
       self.env.current_player == self.human_idx:
        # Get coach recommendation
        rec = self.llm_coach.get_recommendation(
            self.env,
            n_equity_samples=5000
        )
        
        # Display stats
        self.stats_text.insert(f"Equity: {rec['equity']}%")
        self.stats_text.insert(f"Pot Odds: {rec['pot_odds']}%")
        
        # Display LLM recommendation
        self.rec_text.insert(rec['explanation'])
\end{lstlisting}

\textbf{2. Session Statistics:}
Tracks cumulative performance:
\begin{lstlisting}[caption=Session Tracking]
def end_hand(self):
    rewards = self.env.rewards
    
    # Update session stats
    self.session_stats['hands_played'] += 1
    self.session_stats['player_total_bb'] += rewards[0]
    self.session_stats['bot_total_bb'] += rewards[1]
    
    # Color-code display
    color = 'green' if player_total > 0 else 'red'
    self.player_total_label.config(
        text=f"{player_total:+.1f} BB",
        foreground=color
    )
\end{lstlisting}

\textbf{3. Bot Action Visualization:}
Bot decisions are displayed with slight delay for clarity:
\begin{lstlisting}[caption=Bot Action Display]
def bot_action(self):
    # Get bot's decision from CFR strategy
    info_set = self.agent._get_info_set(self.env, bot_idx)
    strategy = self.agent.get_average_strategy(info_set)
    action = sample_from_strategy(strategy)
    
    # Display with 500ms delay
    self.root.after(500, lambda: self.execute_bot_action(action))
\end{lstlisting}

\subsection{User Experience Flow}

Typical hand progression:
\begin{enumerate}
    \item Hand dealt, board shown
    \item If player acts first, buttons enabled + coach displays recommendation
    \item Player clicks action button
    \item Bot responds (with delay for visibility)
    \item Next street dealt or hand ends
    \item Session stats update
    \item "New Hand" button for next hand
\end{enumerate}

\section{Results and Evaluation}

\subsection{Training Curve Analysis}

We compare three training approaches:

\subsubsection{Random Bucketing (Baseline)}

Uses hash-based card abstraction:
\begin{lstlisting}[caption=Simple Hash Card Bucketing]
def simple_bucket(hand, board):
    # Just hash the cards
    hash_val = hash(str(hand) + str(board))
    return hash_val % 50  # Random 50 buckets
\end{lstlisting}

\textbf{Results (50k iterations):}
\begin{itemize}
    \item Final exploitability: ~15 BB/hand
    \item Highly erratic convergence
    \item Agent learns meaningless patterns
\end{itemize}

\textbf{Why it fails:} Hands are grouped arbitrarily. The CFR agent learns which \textit{hash values} are strong, not which \textit{poker hands} are strong.

\subsubsection{K-Means 5k Iterations}

\textbf{Results:}
\begin{itemize}
    \item Final exploitability: ~2.5 BB/hand
    \item Smoother convergence
    \item Info sets learned: 44,390
    \item Training time: 2 hours
\end{itemize}

\textbf{Performance:} Competent but beatable. Makes obvious mistakes in complex spots.

\subsubsection{K-Means 25k Iterations}

\textbf{Results:}
\begin{itemize}
    \item \textbf{Best exploitability: 0.17 BB/hand}
    \item Final exploitability: 0.75 BB/hand
    \item Info sets learned: 45,962
    \item Training time: 7.8 hours
\end{itemize}

\textbf{Performance:} Near-optimal play. Difficult for strong players to exploit.

\subsection{Convergence Analysis}

\subsubsection{Nash Equilibrium Distance}

True Nash equilibrium exploitability in HUNL is estimated at $\approx 0.001$ BB/hand based on Libratus benchmarks.

Our best result (0.17 BB/hand) is approximately:
\begin{equation}
    \frac{0.17}{0.001} = 170\times \text{ Nash exploitability}
\end{equation}

However, this comparison is misleading because:
\begin{enumerate}
    \item Libratus uses no abstraction (blueprint + subgame resolution)
    \item Libratus trained for 15 million core-hours
    \item Our abstraction fundamentally limits achievable optimality
\end{enumerate}

\subsubsection{Information Set Coverage}

Total possible info sets in our abstraction:
\begin{align*}
    |\mathcal{I}| &\approx 169 \times 50 \times 50 \times 10 \times \text{betting sequences} \\
    &\approx 4,225,000 \text{ information sets}
\end{align*}

We explored 45,962 info sets (1.1\% of total). 

\textbf{Implication:} Most game situations weren't encountered during training. This is actually \textit{acceptable} - CFR generalizes through abstraction, and rarely-visited info sets use the default uniform strategy.

\subsubsection{Estimated Iterations for Full Convergence}

Based on info set discovery rate:
\begin{equation}
    \text{Iterations needed} \approx 25000 \times \frac{4,225,000}{45,962} \approx 2.3 \text{ million}
\end{equation}

At current speed (0.9 iter/sec), this would take:
\begin{equation}
    \frac{2,300,000}{0.9 \times 3600} \approx 710 \text{ hours} \approx 30 \text{ days}
\end{equation}

\textbf{Conclusion:} Full convergence is impractical on consumer hardware but unnecessary - our 25k bot already plays at a very high level.

\subsection{Practical Performance}

\textbf{Against Human Players:}
\begin{itemize}
    \item Defeats author (intermediate player) in 73\% of sessions
    \item Defeats author + coach assistance in 58\% of sessions
    \item Makes few exploitable mistakes
    \item Adjusts bet sizing appropriately to pot and board texture
\end{itemize}

\textbf{Coach Quality:}
\begin{itemize}
    \item Equity estimates accurate within ~1\% (validated against exact calculators)
    \item Recommendations align with GTO solver outputs in 85\% of spots tested
    \item Natural language explanations are coherent and strategically sound
    \item Response time: ~1-2 seconds per recommendation
\end{itemize}

\section{Challenges and Limitations}

\subsection{Why Nash Equilibrium Wasn't Achieved}

\subsubsection{Fundamental Limitations}

\textbf{1. Abstraction Granularity:}

Our 50 flop/turn buckets lose information. Consider:
\begin{itemize}
    \item \textbf{Bucket 42:} Top pair with weak kicker, flush draw possible
    \item Maps to same bucket: Top pair good kicker, no flush draw
\end{itemize}

These require different strategies but are treated identically.

\textbf{2. Betting Abstraction:}

We limited to 5 bet sizes. Real optimal play might require:
\begin{itemize}
    \item 37\% pot bet (we use 33\% or 50\%)
    \item 0.87 pot bet (we use 75\% or 100\%)
\end{itemize}

\textbf{3. Computational Budget:}

25,000 iterations is tiny compared to research systems:
\begin{itemize}
    \item Libratus: $> 10^{15}$ hands simulated
    \item Pluribus: $> 10^{12}$ hands simulated
    \item Our system: $2.5 \times 10^{4}$ hands
\end{itemize}

\subsubsection{Why Our Project Still Has Value}

Despite not achieving true Nash equilibrium:

\begin{enumerate}
    \item \textbf{Practical Performance:} 0.17 BB/hand exploitability is excellent for real-world play. Most human players won't exploit errors that small.
    
    \item \textbf{Educational Tool:} The coach provides valuable strategic insights regardless of bot optimality.
    
    \item \textbf{Accessibility:} Runs on consumer hardware, making advanced poker AI accessible.
    
    \item \textbf{Complete System:} End-to-end implementation demonstrating modern AI techniques.
\end{enumerate}

\subsection{Computational Challenges}

\subsubsection{Comparison to Libratus/Pluribus}

\textbf{Libratus (2017):}
\begin{itemize}
    \item Hardware: Pittsburgh Supercomputing Center (100+ nodes)
    \item Training time: Several months
    \item Abstraction: Blueprint + real-time subgame solving
    \item Result: Superhuman play
\end{itemize}

\textbf{Our System:}
\begin{itemize}
    \item Hardware: MacBook Pro M1 (8 cores)
    \item Training time: 8 hours
    \item Abstraction: Fixed K-Means + action abstraction
    \item Result: Strong amateur-level play
\end{itemize}

\textbf{Key Differences:}
\begin{enumerate}
    \item \textbf{No Endgame Solving:} Libratus re-solves endgame in real-time with full granularity. We use fixed strategy.
    
    \item \textbf{Blueprint Refinement:} Libratus iteratively refines abstraction in commonly-reached subgames. We use static abstraction.
    
    \item \textbf{Safe Subgame Solving:} Pluribus introduced bounded subgame solving to prevent exploitation. We don't handle this.
\end{enumerate}

\subsection{Obstacles During Development}

\subsubsection{Initial Random Bucketing Failure}

\textbf{Problem:} First implementation used:
\begin{lstlisting}
bucket = hash(str(hand) + str(board)) % 50
\end{lstlisting}

\textbf{Result:} Agent "learned" that certain hash values beat others, not actual poker strategy. Played terribly.

\textbf{Solution:} Implemented K-Means on equity distributions, which groups strategically similar hands.

\subsubsection{Environment State Bugs}

\textbf{Problem:} \texttt{street\_investment} vs \texttt{street\_bets} naming mismatch between environment and coach caused crashes.

\textbf{Solution:} Standardized on \texttt{street\_investment} across all components.

\subsubsection{Training Speed}

\textbf{Problem:} Initial training speed was ~0.1 iterations/second (70 hours for 25k iterations).

\textbf{Solutions:}
\begin{enumerate}
    \item Eliminated unnecessary deep copying of deck (100x speedup)
    \item Reduced Monte Carlo samples during clustering (10x speedup)
    \item Optimized list copying with slicing (1.2x speedup)
\end{enumerate}

Final speed: 0.9 iterations/second (7.8 hours for 25k).

\section{Future Work and Differences with Libratus}

\subsection{Potential Improvements}

\subsubsection{Algorithmic Enhancements}

\textbf{1. CFR+ Variant:}
CFR+ uses optimistic regret matching for faster convergence:
\begin{equation}
    R^{T+1}(I, a) = \max\left(R^T(I,a) + r^{T+1}(I,a), 0\right)
\end{equation}

Expected improvement: 2-3x faster convergence.

\textbf{2. Monte Carlo CFR:}
Sample chance nodes (board cards) in addition to opponent actions. Enables larger abstraction spaces.

\textbf{3. Deep CFR:}
Use neural networks to approximate value functions, eliminating need for explicit abstraction \cite{brown2019deep}.

\subsubsection{Abstraction Refinements}

\textbf{1. Adaptive Bucketing:}
Use more buckets for commonly-reached game states, fewer for rare states.

\textbf{2. Imperfect Recall Abstraction:}
Merge similar earlier streets to reduce info set count while preserving crucial distinctions.

\textbf{3. Opponent-Aware Abstraction:}
Adjust abstraction based on observed opponent tendencies (exploit weak players).

\subsubsection{System Enhancements}

\textbf{1. Multi-Player Support:}
Extend to 6-max or 9-max games using techniques from Pluribus.

\textbf{2. Web Deployment:}
Create browser-based interface for wider accessibility.

\textbf{3. Historical Hand Analysis:}
Allow users to upload hand histories for post-hoc coaching.

\textbf{4. Opponent Profiling UI:}
Display tracked opponent stats and adjust recommendations accordingly.

\subsection{Key Differences from Libratus}

\begin{table}[h]
\centering
\caption{Libratus vs Our System}
\begin{tabular}{|l|p{5cm}|p{5cm}|}
\hline
\textbf{Aspect} & \textbf{Libratus} & \textbf{Our System} \\
\hline
Abstraction & Blueprint + real-time solving & Static K-Means \\
\hline
Compute & Supercomputer & Consumer laptop \\
\hline
Training Time & Months & 8 hours \\
\hline
Exploitability & ~0.001 BB/hand & ~0.17 BB/hand \\
\hline
Opponent Modeling & Adversarial learning & Basic stat tracking \\
\hline
Endgame & Full resolution & Fixed strategy \\
\hline
Accessibility & Research only & Open source \\
\hline
Coaching & None & LLM-powered \\
\hline
\end{tabular}
\end{table}

\textbf{Fundamental Tradeoff:}
Libratus optimizes for maximum strength regardless of cost. Our system optimizes for best performance per compute dollar, plus educational value through coaching.

\section{Conclusion}

\subsection{What I Learned}

\subsubsection{Technical Learnings}

\textbf{1. Game Theory is Powerful:}
CFR's mathematical guarantees are remarkable - given enough iterations, Nash equilibrium emerges automatically from self-play. No human poker knowledge is explicitly encoded.

\textbf{2. Abstraction is Critical:}
The difference between random bucketing (15 BB/hand exploitability) and K-Means (0.17 BB/hand) is dramatic. \textit{How} you reduce the problem space matters as much as the algorithm itself.

\textbf{3. LLMs Enable New Interfaces:}
Natural language coaching makes poker strategy accessible to non-experts. The LLM translates mathematical analysis (equity, pot odds) into strategic advice.

\textbf{4. Optimization Matters:}
Simple optimizations (removing copied deck) provided 100x speedups. Always profile before optimizing, but when you find the bottleneck, fix it.

\subsubsection{Broader Insights}

\textbf{1. Research → Practice Gap:}
Papers describe optimal solutions, but practical systems require tradeoffs. Our "good enough" solution on consumer hardware is more useful than a perfect solution requiring a supercomputer.

\textbf{2. AI Teachability:}
The coaching system demonstrates AI's potential as an educational tool. It doesn't just play well - it explains \textit{why}.

\textbf{3. Integration Complexity:}
Building a complete system (game engine, training, GUI, LLM integration) revealed challenges papers don't discuss: debugging environment state, managing checkpoints, handling edge cases.

\subsection{How This Helps Poker Players}

\subsubsection{Strategy Learning}

The coach provides:
\begin{enumerate}
    \item \textbf{Equity-Based Decisions:} Teaches players to think in terms of win probability, not "gut feel"
    \item \textbf{Position Awareness:} Highlights how IP/OOP changes optimal strategy
    \item \textbf{SPR Understanding:} Explains commit-or-fold decisions in short-stack scenarios
    \item \textbf{Board Texture Recognition:} Identifies draw-heavy boards requiring caution
\end{enumerate}

\subsubsection{Pattern Recognition}

By playing many hands with recommendations, players internalize:
\begin{itemize}
    \item When to fold marginal hands
    \item Appropriate bet sizing for board texture
    \item Balancing value bets and bluffs
    \item Pot odds calculation
\end{itemize}

\subsubsection{Risk Tolerance}

The three risk levels teach situational adaptation:
\begin{itemize}
    \item \textbf{Conservative:} For tournaments (ICM pressure)
    \item \textbf{Moderate:} For balanced cash game play
    \item \textbf{Aggressive:} For exploiting passive opponents
\end{itemize}

\subsection{If I Had the Correct Tools}

Given unlimited resources, I would:

\subsubsection{Computational Resources}

\textbf{1. Cloud GPU Cluster:}
\begin{itemize}
    \item Train for 1+ million iterations
    \item Use finer abstraction (100 flop buckets, 100 turn buckets)
    \item Reach 0.01 BB/hand exploitability
    \item Estimated cost: \$500-1000 on AWS
\end{itemize}

\textbf{2. Distributed Training:}
\begin{itemize}
    \item Parallelize CFR across multiple machines
    \item Reduce training time from hours to minutes
    \item Enable rapid experimentation with abstractions
\end{itemize}

\subsubsection{Advanced Techniques}

\textbf{1. Deep Learning Integration:}
\begin{itemize}
    \item Train neural network to predict counterfactual values
    \item Eliminate explicit abstraction
    \item Handle continuous bet sizing
\end{itemize}

\textbf{2. Real-Time Subgame Solving:}
\begin{itemize}
    \item Compute exact strategy for current subtree
    \item Fall back to blueprint for early streets
    \item Match Libratus's two-stage approach
\end{itemize}

\textbf{3. Opponent Adaptation:}
\begin{itemize}
    \item Detect opponent weaknesses (over-folding, over-calling)
    \item Dynamically adjust strategy to exploit
    \item Would require game-theoretic safeguards to avoid counter-exploitation
\end{itemize}

\subsubsection{User Experience}

\textbf{1. Advanced Analytics:}
\begin{itemize}
    \item Heat maps of equity distributions
    \item Hand range visualizations
    \item Historical performance tracking
    \item Comparative analysis vs GTO solvers
\end{itemize}

\textbf{2. Multi-Modal Coach:}
\begin{itemize}
    \item Voice interface for hands-free coaching
    \item Video analysis of recorded game play
    \item Personalized training regimens based on leaks
\end{itemize}

\textbf{3. Mobile Deployment:}
\begin{itemize}
    \item iOS/Android apps
    \item Offline play vs bot
    \item Online coaching during real poker sessions
\end{itemize}

\subsection{Final Thoughts}

This project demonstrates that cutting-edge AI techniques are increasingly accessible. A motivated individual with consumer hardware can build systems that, while not matching billion-dollar research labs, still achieve impressive results.

The combination of classical game theory (CFR) with modern NLP (LLMs) represents a promising direction: AI that doesn't just perform tasks but \textit{teaches} humans to perform them better.

Poker is a microcosm of strategic decision-making under uncertainty - skills valuable far beyond the card table. Building this system deepened my understanding of:
\begin{itemize}
    \item Mathematical optimization
    \item Algorithm design and analysis
    \item System integration
    \item User experience design
    \item The gap between theory and practice
\end{itemize}

Most importantly: perfect is the enemy of good. Our 0.17 BB/hand bot isn't optimal, but it's \textit{good enough} to beat most humans and teach valuable lessons. That's a success.

% References
\begin{thebibliography}{9}

\bibitem{brown2017libratus}
Brown, N., \& Sandholm, T. (2017). Superhuman AI for heads-up no-limit poker: Libratus beats top professionals. \textit{Science}, 359(6374), 418-424.

\bibitem{brown2019pluribus}
Brown, N., \& Sandholm, T. (2019). Superhuman AI for multiplayer poker. \textit{Science}, 365(6456), 885-890.

\bibitem{moravcik2017deepstack}
Moravčík, M., Schmid, M., Burch, N., Lisý, V., Morrill, D., Bard, N., ... \& Bowling, M. (2017). DeepStack: Expert-level artificial intelligence in heads-up no-limit poker. \textit{Science}, 356(6337), 508-513.

\bibitem{zinkevich2007regret}
Zinkevich, M., Johanson, M., Bowling, M., \& Piccione, C. (2007). Regret minimization in games with incomplete information. \textit{Advances in Neural Information Processing Systems}, 20.

\bibitem{brown2019deep}
Brown, N., Lerer, A., Gross, S., \& Sandholm, T. (2019). Deep counterfactual regret minimization. In \textit{International Conference on Machine Learning} (pp. 793-802). PMLR.

\bibitem{lanctot2009monte}
Lanctot, M., Waugh, K., Zinkevich, M., \& Bowling, M. (2009). Monte Carlo sampling for regret minimization in extensive games. \textit{Advances in Neural Information Processing Systems}, 22.

\end{thebibliography}

\appendix

\section{Code Repository}

Full source code available at: \texttt{https://github.com/PuffBear/turnsports-poker-ai}

\section{Training Curves}

[Include figure: checkpoints/cfr\_kmeans/training\_curves\_50k.png]

\section{Sample Coach Interaction}

\begin{verbatim}
=== HAND SITUATION ===
Street: Turn
Your Hand: Ah Kd
Board: Qh Js 4c 2h
Position: Button (IN POSITION ✓)

=== EQUITY ANALYSIS ===
Win Probability: 45.3%
Required Equity: 28.6%
✓ Strong Equity Edge: +16.7% (profitable call)

Recommendation: Call here is correct. Your gutshot 
straight draw plus two overcards gives you ~10 outs
(any T, any A, any K), translating to 45% equity.
This is well above the 28.6% needed to call profitably.
Being in position, you can see the river cheaply and
decide whether to value bet if you hit or bluff if
you miss and your opponent checks.
\end{verbatim}

\end{document}
